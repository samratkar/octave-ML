Sometimes cost function even reduced via gradient descent, and causing a very high accuracy with
training data, might fare bad with the test data. This is a fallout of overfitting. This needs to be
resolved. Following are the ways in which such situations are handled. But one needs to run diagnostics and
evaluation of their model to identify which way to go. The possibilities are -
1. Increase the data set.
2. Decrease the number of features. Consider only the ones that are important, i.e has low p value.
3. Try getting other features instead of the current ones, which might have low p value.
4. Adjust lamda to manage the over fitting
5. Try out polynomials. This might however cause over fitting too.

------------------------------------------------------------------------------------------------------------------
                                Evaluating the model
------------------------------------------------------------------------------------------------------------------

A - Test and Train sub sets
............................................
1. Randomly divide the training set to 70% training and 30% test data.
2. Learn the thetas from the 70% training set, by minimizing the cost function J(theta) on only the training data, using gradient descent.
3. Find the test error. J(theta) on only the test data.
4. For the classification problems, you can find the J(theta) for the test set as above using the J(theta) of logistic function.
   Or u can use the 0/1 "misclassification error". This is basically the average of sum of all errors for the opposite of sigmoid on every test set

B - Selecting the degree of polynomial
.............................................
Find the Jtest(theta) for d=1, 2, 3, 4...and then find the Jtest() which is the lowest. But again the same issue lies. We based the
test on the test set, and for this test set, d might be optimum. What is the guarantee that it will be optimum for other test sets?
This is solved by randomly dividing the trainings set into three subset -
    i. Training set - 60% (mtrain)
    ii. Cross Validation set - 20% (mcv)
    iii. Test set - 20% (mtest)

Then follow the following -
    1. Use training set to identify theta using gradient descent. [**use the traditional cost functions**]
    2. Use cross validation set to identify the correct order of the equation of the hypothesis.
    3. Use the test set to identify the error of the hypothesis.
        i. ****SIMPLE REGRESSION ERROR FUNCTION - SAME AS TRADITIONAL COST FUNCTION****
           For regression issues the error can be measured directly by the cost function.
           This is because, the error is absolutely proportional to the cost function.
        ii.****LOGISTIC REGRESSION OR NEURAL NETWORK ERROR FUNCTION*****
           For the logistic regression, the logarithmic cost function that we have gives a measure of whether
           we are nearing the actual target or not. But it does not give precise errors. This is because
           the target is a binary value. Same should be the h(theta). Rather h(theta) is the sigmoid method which
           gives probabilities. Now, if h(theta)>0.5, say some probability value as 0.8. For that, hypothesis for a given
           training set say, if y=1, we have the cost function as follows -
            J(theta)= 1/m(-log h) = 1/m(-log 0.8) = 1/m (0.0969)
            Please note that there is a finite cost still in this case, although it is a very small number. Certainly
            in this case, error should have been zero, as any hypothesis h(x) > 0.5 is a region which should be
            predicted as y=1. And similarly for any hypothesis with h(theta) < 0.5, the region should be predicted as y=0.
            Any discrepancy in this predicted y and the actual y, should give the error.

            Hence there is a need of a separate function for logistic regression to find the misclassification fractions of all
            the test set. This is given by the function -
            err(hΘ(x),y) =1 if hΘ(x)≥0.5 and y=0 or hΘ(x)<0.5 and y=1
            err(hΘ(x),y) = 0 otherwise
            This gives us a binary 0 or 1 error result based on a misclassification. The average test error for the test set is:

            Test Error=1/mtest∑err(hΘ(x(i)test),y(i)test)


C - Under-fitting (high bias) and Over-fitting (high variability) problem
.......................................................................
There are two reasons why you are getting high error - either your model has a high variation or high bias. The idea is to find
which one is the case, and take corrective action accordingly.
Note that when you plot the error method with respect to degree of polynomials the error values keep
decreasing with the increase in the degree of polynomials, if you consider the training set. That has to
happen as you would have already run gradient descent.
But then, when you plot the error function with the degree of polynomial, considering the cross validation set,
it is like a bowl. The error will be high when the order is low. That is the area of bias. The error will keep
reducing with increase in polynomial, which is expected. But then at a point due to overfitting it might
not be able to predict accurately few of the cross validation sets. That area of high error is known as
area of high variability.
