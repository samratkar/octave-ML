Sometimes cost function even reduced via gradient descent, and causing a very high accuracy with
training data, might fare bad with the test data. This is a fallout of overfitting. This needs to be
resolved. Following are the ways in which such situations are handled. But one needs to run diagnostics and
evaluation of their model to identify which way to go. The possibilities are -

1. Increase the data set. [FIXES HIGH VARIANCE, BUT NOT HIGH BIAS]
2. Decrease the number of features. Consider only the ones that are important, i.e has low p value. [FIXES HIGH VARIANCE, BUT NOT HIGH BIAS]
3. Try getting other ADDITIONAL features. [FIXES HIGH BIAS, BUT NOT HIGH VARIANCE]
4. Adjust lamda to manage the over fitting
5. Try out polynomials. This might however cause over fitting too. [FIXES HIGH BIAS, BUT NOT HIGH VARIANCE]
6. Try decreasing lambda [FIXES HIGH BIAS]
7. Try increasing lambda [FIXES HIGH VARIANCE]

NEURAL NETWORKS
It is always better to create a huge neural network with some lambda value, rather than small neural network.
You will always have to arrive at the point to decide whether there is a high variance or high bias for your neural networks.
This can be ascertained by the nature of the Jcv and Jlearning. Jcv and Jlearning being very high, but close to each other
signify high bias. A low Jlearning and a huge difference between Jcv and Jlearning where Jcv is high, implies a high variance.
If the variance is already very high, it will not help in increasing the hidden layers or units. You might have to increase lambda.

------------------------------------------------------------------------------------------------------------------
                                Evaluating the model
------------------------------------------------------------------------------------------------------------------

A - Test and Train sub sets
............................................
1. Randomly divide the training set to 70% training and 30% test data.
2. Learn the thetas from the 70% training set, by minimizing the cost function J(theta) on only the training data, using gradient descent.
3. Find the test error. J(theta) on only the test data.
4. For the classification problems, you can find the J(theta) for the test set as above using the J(theta) of logistic function.
   Or u can use the 0/1 "misclassification error". This is basically the average of sum of all errors for the opposite of sigmoid on every test set

B - Selecting the degree of polynomial
.............................................
Find the Jtest(theta) for d=1, 2, 3, 4...and then find the Jtest() which is the lowest. But again the same issue lies. We based the
test on the test set, and for this test set, d might be optimum. What is the guarantee that it will be optimum for other test sets?
This is solved by randomly dividing the trainings set into three subset -
    i. Training set - 60% (mtrain)
    ii. Cross Validation set - 20% (mcv)
    iii. Test set - 20% (mtest)

Then follow the following -
    1. Use training set to identify theta using gradient descent. [**use the traditional cost functions**]
    2. Use cross validation set to identify the correct order/lambda of the equation of the hypothesis.
    3. Use the test set to identify the error of the hypothesis.
        i. ****SIMPLE REGRESSION ERROR FUNCTION - SAME AS TRADITIONAL COST FUNCTION****
           For regression issues the error can be measured directly by the cost function.
           This is because, the error is absolutely proportional to the cost function.
        ii.****LOGISTIC REGRESSION OR NEURAL NETWORK ERROR FUNCTION*****
           For the logistic regression, the logarithmic cost function that we have gives a measure of whether
           we are nearing the actual target or not. But it does not give precise errors. This is because
           the target is a binary value. Same should be the h(theta). Rather h(theta) is the sigmoid method which
           gives probabilities. Now, if h(theta)>0.5, say some probability value as 0.8. For that, hypothesis for a given
           training set say, if y=1, we have the cost function as follows -
            J(theta)= 1/m(-log h) = 1/m(-log 0.8) = 1/m (0.0969)
            Please note that there is a finite cost still in this case, although it is a very small number. Certainly
            in this case, error should have been zero, as any hypothesis h(x) > 0.5 is a region which should be
            predicted as y=1. And similarly for any hypothesis with h(theta) < 0.5, the region should be predicted as y=0.
            Any discrepancy in this predicted y and the actual y, should give the error.

            Hence there is a need of a separate function for logistic regression to find the misclassification fractions of all
            the test set. This is given by the function -
            -----------------------------------
            MISCLASSIFICATION FUNCTION
            -----------------------------------
            err(hΘ(x),y) =1 if hΘ(x)≥0.5 and y=0 or hΘ(x)<0.5 and y=1
            err(hΘ(x),y) = 0 otherwise
            This gives us a binary 0 or 1 error result based on a misclassification. The average test error for the test set is:

            Test Error=1/mtest∑err(hΘ(x(i)test),y(i)test)


C - Under-fitting (high bias) and Over-fitting (high variability) problem
.......................................................................
There are two reasons why you are getting high error - either your model has a high variation or high bias. The idea is to find
which one is the case, and take corrective action accordingly.
Note that when you plot the error method with respect to degree of polynomials the error values keep
decreasing with the increase in the degree of polynomials, if you consider the training set. That has to
happen as you would have already run gradient descent.
But then, when you plot the error function with the degree of polynomial, considering the cross validation set,
it is like a bowl. The error will be high when the order is low. That is the area of bias. The error will keep
reducing with increase in polynomial, which is expected. But then at a point due to overfitting it might
not be able to predict accurately few of the cross validation sets. That area of high error is known as
area of high variability.

D - LEARNING CURVE
------------------------------------------
The performance of the J(theta)cv and J(theta)train wrt to the increase of training set is known as learning curve.
1. As training set is low, Jtrain is low. Because it is easier to fit less points. Jcv will be high, because it is
unaware of the training set. As you increase number of training set, the Jcv decreases. But then the Jtrain keeps increasing,
till it stabilizes.
When there is a high bias, the Jtrain would remain constance at a high value. So, by increasing the training set
will have no value in decreasing the cost function.

NOTE: The absolute values of the Jcv, Jtrain, Jtest cannot be predicted, and their relative values do not say much. It can be
in any way. Only the following can be adhered to -
1. If the Jtrain progresses close to Jtest and Jcv, know that there is a high bias.
2. If the Jtrain progresses away from Jtest and Jcv, know that there is high variance.

E - EVALUATING A SPAM CLASSIFIER
-------------------------------------------
1. Make the features as the high repeated words across several spam emails.
2. Create highly sophisticated feature engineering by taking the header information, or processing the email body, etc of the spam email.
3. Correct the spelling of the spam emails. But this might not yield much result. Because if the mispellings are less
   correcting them will not matter. And if misspellings are more that can ideally be a new feature.
   But yes, if there are unique misspellings, it might make sense to correct the misspellings.

F - ERROR ANALYSIS
--------------------------------------------
1. Build and easy machine learning system with simple feature matrix and see how it peforms.
2. Plot the learning curves and identify whether your algorithm suffers from high bias or high variance. Based on that
   take definitive steps as either increasing the number of training set, or number of features, or order of the features.
3. Manually examine the examples in cross validation set, that your algorithm made errors on. See if you can make any
   systematic trend in what type of examples it is making errors on.

G - HANDLING SKEWED DATA [ONLY TWO LABELS - 0 AND 1]
-------------------------------------------------------
Skewed data is an issue with the training/test/cv data points such that a very small percentage of the data is actually
violating/agreeing to the hypothesis. In that case we actually do not need to predict anything, as probability of
one label is very high. Such a case is known as skewed data.

We use terms as follows to depict the combination of how the predicted and actual data shows up -
[Actual Result -> TRUE/FALSE] [Predicted result -> POSITIVE/NEGATIVE]

             ACTUAL CLASS
            1             0
P     ---------------------------
R C   |   True      |  False    |
E L 1 |   Positive  |  Positive | ==> All the cases of positive predictions.
D A   |-------------------------|
I S 0 |  False      | True      | ==> All the cases of negative predictions.
C S   |  Negative   | Negative  |
T     ---------------------------
        True cases    False cases

1. Precision = # "TRUE" "POSITIVES" / (# "TRUE" "POSITIVES" + # "FALSE" "POSITIVES" )
[Of ALL the patients we have PREDICTED has cancer, what fraction that we "CORRECTLY" "PREDICT" as having cancer]
[This gives the fraction of "True-Positives", for all the cases of "Positives". Precision is all about the positive predictions. It does not say anything about the negative predictions. So, it just gives you the power of the algorithm only for the cases which are predicted positively]
[PRECISION is a measure of POSITIVE PREDICTIONS which are TRUE in the crowd of all the POSITIVE PREDICTIONS.]

2. Recall = # "TRUE" "POSITIVES" / # "TRUE"
          = # "TRUE" "POSITIVES" / (# "TRUE" "POSITIVES" + "FALSE" "NEGATIVES")
[Of ALL the patients that actually have cancer (TRUE), what fraction did we correctly PREDICT AS POSITIVE as having cancer] 

