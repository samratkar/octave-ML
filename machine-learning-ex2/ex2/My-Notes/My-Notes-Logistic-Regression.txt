-------------------------------------------------------------
BOUNDARY CONDITION / BOUNDARY EQUATION / BOUNDARY VALUES
-------------------------------------------------------------
Logistic regression is all about classification. Hence it is all about identifying the most optimal boundary
condition / boundary equation / boundary values, that classify the positives and negatives correctly.
When we identify the regression equation in a logistic regression, we are identifying exactly this boundary
condition/equation.

THe same is true for SVMs and neural nets. Gradient descent is all about optimization of theta so that we
finally determine a boundary equation. This is IMPORTANT.




----------------------------------------------------------------
Cost function of logistic regression
----------------------------------------------------------------

J(theta) = - log(h(theta)) , for y = 1 (exponentially decreasing)
J(theta) = - log(1-h(theta)), for y = 0 (exponentially increasing).
NOTE -
When we plot J(theta) vs h(theta), the value of J will be in the range [0:1].
Range of log is (-infinity, +infinity) but in this case the range of the cost function
will be 0 to 7. This is because h(theta) is basically g(z), which is sigmoid(z),
whose value is in turn in the range [0:1]. So for, that domain, the range is also limited.
But the point to be notes is that in the domain [0:1] log(x) is negative. It ranges from around
-4 to 0. log(1) is zero.
A negative cost function does not make sense. So, we use a negative term and make the cost function as
J(theta) = 1/m sigma(-log(h(x))) for y = 1
J(theta) = 1/m sigma(-log(1-h(x))) for y = 0
Putting them together the cost function becomes -
J = 1/m(sigma(-y*log(h) - (1-y) * log(1-h)))
So till now, we have agreed that it should be -log(x). But now the question is why log?
And how this function would actually find the error in prediction?

To understand this we plot h(theta) against J(theta), and see what happens as h(theta) increases
and decreases. This is implemented in Logistic_regression.m
If you see the graph it is of the shape of a glass. If y=1, as the h(theta) which is the actually
the probability that the predicted value is 1, increases, the cost function decreases proportionately.
Also as the h(theta) increases, and y = 0, the cost function increases. This gives an intuition that the
cost function is good enough.

But, this increase and decrease of cost function automatically with the switching of y's, with a
linear increase in h(theta) (probability of predicted value = 1) needs to be proportional to the
increase or decrease of h(theta). If you see the graph this is also pretty intuitive. The wine glass
graph, for most of the regions is a almost a proportional line passing through origin. That is the test for
proportionality. This is almost proportional and it is not parabolic.
That is the beauty of log methods.


######################################################################################################
            VERY VERY IMPORTANT - COST FUNCTION OF LOGISTIC REGRESSION IS INDIRECTLY FRAMED.
                 The need of another 0/1 mis-classification function "Test Error"
######################################################################################################
The logarithmic Cost Function of logistic regression has nothing to do with the actual values of the target
values of the actual experiments - y.
The equation below of cost function has used y just as a switch to turn to the right forumula based on
what is the value of target y. The cost function per-se does not even care for what is the target y and
how far it is with the computed cost function. We are just leveraging the nature of the log function
in this case, which happens to be an independent function, that in form appears a close approximation
of the cost function.

J(theta) = 1/m sigma(-log(h(x))) for y = 1
J(theta) = 1/m sigma(-log(1-h(x))) for y = 0
Putting them together the cost function becomes -
J = 1/m(sigma(-y*log(h) - (1-y) * log(1-h)))
if we vectorize this, it will become -
J = 1/m ( -y' * log(h(x)) - (1-y)' * log(1-h(x)) )
=> J = 1/m * (-y' * log(sigmoid(X * theta)) - (1-y)' * log(1-(sigmoid(X*theta))));

Due to this uniqueness of the cost function, we will continue to use it to train the model to identify
the correct theta matrix. But then it will never give the exact precise times the model is failing on the
y value. To determine the error or 0/1 mis-classification we follow the following function for logistic regression -

err(hΘ(x),y) =1 if hΘ(x)≥0.5 and y=0 or hΘ(x)<0.5 and y=1
err(hΘ(x),y) = 0 otherwise
This gives us a binary 0 or 1 error result based on a misclassification. The average test error for the test set is:

Test Error=1/mtest∑err(hΘ(x(i)test),y(i)test)

This gives us the proportion of the test data that was misclassified.
