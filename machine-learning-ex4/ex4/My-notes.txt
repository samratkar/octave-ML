-----------------------------------------------------
PUTTING ALL TOGETHER TO CREATING A NEURAL NETWORK
-----------------------------------------------------

I. Neural Network Architecture
.......................................
1. Pick a network architecture. The number of nodes in the output layer is alway
same as number of labels to which you want to classify. Number of nodes of the input layer
is equal to number of features.
2. The size (s) of each of the hidden layers needs to be ascertained. Most common is to use
only one hidden layer.
3. Number of units (nodes) in each hidden layer is same! More the number of hidden units, better it is.


II. Training a neural Network
....................................
1. Randomly initialize the weights using epsilon based on the number of input and output units
2. Implement the forward propagation
3. Implement the cost function
4. Implement the backward propagation to compute the partial derivatives.
5. Use gradient checking to ensure that the gradient descent algo is working. THen disable gradient check
6. Use gradient descent to optimize the values of theta.





----------------------------------------------------------------------------------------
COST FUNCTION FOR LOGISTIC REGRESSION AND NEURAL NETWORK EMPLOYING LOGISTIC REGRESSION
----------------------------------------------------------------------------------------

1. IN logistic regressions, there is always a transformation of the y to to a new "transformed Y",
that is populated to encode the different labels.
Y TRANSFORMATION - In neural networks, the one thing that is most important is "Y TRANSFORMATION".
There is a key difference between normal logistic regression and neural networks employing logistic
regression. In the normal case, we have just one h(theta). So, we need to just find the difference of h(theta)
with that of y, to determine the cost function. It is true that you need to do some transformation of the y
to show up the labels. But you do not need to get into the comlexity of different h(theta). You might
have different labels. But then it is still simple because h(theta) is same. So, after encoding the y,
you can just find the cost function direction as follows -

While computing the optimal theta using gradient descent you use the following -

for c = 1:num_labels
 [theta, J, exit_flag] = fmincg (@(t)(lrCostFunction(t, X, (y == c), lambda)), initial_theta, options);
 all_theta(c,:) = theta;
endfor

Note - the third parameter is a transformed y vector, based on the label. But the point is that y still remains
a vector. Cost function and gradient looks like the following -

J = 1/m * (-y' * log(sigmoid(X * theta)) - (1-y)' * log(1-(sigmoid(X*theta)))) + (lambda/(2*m) * sum(theta(2:n).^2));
grad = 1/m * X' * (sigmoid(X * theta) - y);
grad (2:n) = grad (2:n) + lambda/m * theta (2:n) ;

....................
Neural Network
....................
In case of neural network, there are multiple h(theta). TO be able to find the cost of a given h(theta), you
need its corresponding y. So, along with the transformation of y to encode the labels, you also need a means to
classify all the y's belonging to one particular h(theta) so that when you do  a matrix multiplication of the
h(theta) of the newly transformed y, only those members of y are picked up which belong to that particular
label represented by the corresponding h(theta). Both of these can be easily done using identity matrix transformation.
Identity matrix works as an encoder, where each row acts as an encoded mapping of each label. You create
an identity matrix of the size of that of labels. You scoop out appropriate row from this identity matrix,
and keep stacking it in the transformed Y. The number of row in this transformed Y, is same as the number
of training set. Each row has a pattern of all zeros and one 1 to encode a unique label.
It looks something like this -

m = data size.
I = eye(num_labels)
for i = 1 : m
    Y(i,:) = I(y(i), :)
endfor

The special thing about this transformed Y, is that if you do a matrix multiplication of h(theta) with Y,
while determining the cost function, all the non relevant values for h(theta) for a particular row of Y,
will return zero, and hence will be ignored.

Here is the algorithm for the cost function determination -
*************************************************************************
%Identify matrix encoder
I = eye(num_labels); %10 * 10 matrix of Identity encoder. it is always num_labels * num_labels
%Transforming the y into encoded Y with the help of the identity encoder.
Y = zeros(m, num_labels); % 5000 * 10 matrix
for i=1:m
  Y(i, :)= I(y(i), :);
end


A1 = [ones(m, 1) X]; % 5000 * 401 matrix
%Theta1 = 25*401 matrix. So Theta1' is 401*25 matrix. So Z2 = 5000*25 matrix
Z2 = A1 * Theta1'; % 5000 * 25 matrix
A2 = [ones(size(Z2, 1), 1) sigmoid(Z2)]; % 5000*26 matrix
% Theta2 is 10*26 matrix. So Theta2' is 26*10 matrix
Z3 = A2*Theta2'; % 5000*10 matrix
H = A3 = sigmoid(Z3); % 5000*10 matrix


penalty = (lambda/(2*m))*(sum(sum(Theta1(:, 2:end).^2, 2)) + sum(sum(Theta2(:,2:end).^2, 2)));

J = (1/m)*sum(sum((-Y).*log(H) - (1-Y).*log(1-H), 2));
J = J + penalty;
***************************************************************************

----------------------------------
Backward propagation
----------------------------------
Here is the code for the backward propagation -
************************************************************************************
% BACKWARD PROPAGATION
Sigma3 = A3 - Y; % 5000*10 matrix
Sigma2 = (Sigma3*Theta2 .* sigmoidGradient([ones(size(Z2, 1), 1) Z2]))(:, 2:end);


Delta_1 = Sigma2'*A1;
Delta_2 = Sigma3'*A2;


Theta1_grad = Delta_1./m + (lambda/m)*[zeros(size(Theta1,1), 1) Theta1(:, 2:end)];
Theta2_grad = Delta_2./m + (lambda/m)*[zeros(size(Theta2,1), 1) Theta2(:, 2:end)];
************************************************************************************
